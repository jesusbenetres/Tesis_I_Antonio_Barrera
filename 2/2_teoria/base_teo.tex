\subsection{Introducción al Deepfake de Audio}
La tecnología deepfake ha revolucionado la creación y manipulación de contenido audiovisual mediante la aplicación de algoritmos de aprendizaje profundo. Un deepfake se define como una forma avanzada de manipulación digital que utiliza redes neuronales profundas, principalmente redes generativas adversarias (GAN), para crear contenido falso, como imágenes, videos o audios, que pueden engañar a los espectadores al simular la apariencia o voz de una persona real (Heidari, Jafari Navimipour, Dag, \& Unal, 2023). En particular, los deepfakes de audio han cobrado relevancia debido a su capacidad para clonar voces, reproduciendo patrones vocales como el tono, el timbre y la prosodia. Estos avances han sido utilizados tanto para fines de entretenimiento como para actividades maliciosas, como el fraude y la suplantación de identidad, especialmente en el contexto de la voz (Rojas Berríos, 2023).

El uso de deepfakes de audio en esquemas de fraude ha aumentado en países como Perú, donde se han reportado numerosos casos en los que los delincuentes emplean inteligencia artificial para replicar la voz de familiares de las víctimas. Utilizando modelos de texto-a-voz y conversión de voz, estos audios logran imitar de manera realista la voz de una persona, engañando a las víctimas y logrando que envíen dinero bajo falsas emergencias. Esta técnica permite a los estafadores clonar con precisión voces a partir de grabaciones previas, aprovechando así la confianza de las víctimas en la veracidad de la voz escuchada (Oorloff et al., 2024; Al-Adwan, Alazzam, Al-Anbaki, \& Alduweib, 2024).

\subsection{Teoría y Fundamentos en Análisis de Voz}
El análisis de voz se enfoca en descomponer y analizar las señales de audio para obtener características distintivas de cada persona. Estos análisis incluyen parámetros acústicos como el tono (pitch), que es la frecuencia fundamental de la voz, y el timbre, que describe la "calidad" de la voz que hace que cada persona suene única. Estas características son esenciales para identificar posibles manipulaciones en deepfakes de audio, ya que los modelos de síntesis de voz pueden tener dificultades para replicar estos parámetros con precisión (Yi, Wang, Tao, Zhang, \& Zhao, 2023).

El tono y el timbre son solo algunos de los componentes importantes en el análisis de voz para la detección de deepfakes. Otros parámetros clave incluyen el ritmo, la intensidad, y la prosodia, que se refiere a la entonación y las variaciones rítmicas en el habla. La prosodia, por ejemplo, puede ser compleja de replicar en modelos de síntesis de voz, ya que implica variaciones naturales que dependen del estado emocional y del contexto de la comunicación. Estas características acústicas y su análisis en el dominio del tiempo y la frecuencia proporcionan un marco para la detección de manipulaciones, permitiendo la identificación de patrones no naturales en audios generados artificialmente (Heidari et al., 2023; Lanzino et al., 2024).

\subsection{Teoría y Fundamentos en Análisis de Voz}

La detección de deepfakes ha avanzado significativamente con el desarrollo de redes neuronales profundas, que son capaces de aprender patrones complejos en datos no estructurados como el audio. Las redes neuronales convolucionales (CNN) y las redes neuronales recurrentes (RNN) son modelos ampliamente utilizados para procesar datos secuenciales, como el audio, y extraer características relevantes para clasificar los audios en reales o falsos (Groh, Sankaranarayanan, Singh, Kim, Lippman, \& Picard, 2024). Las CNN son efectivas en la extracción de patrones de frecuencia y textura, mientras que las RNN, especialmente los modelos de memoria a corto y largo plazo (LSTM), permiten capturar dependencias temporales en la señal de audio.

Recientemente, los investigadores han explorado modelos híbridos que combinan CNN y RNN junto con técnicas de optimización, como el algoritmo de Optimización por Enjambre de Partículas (PSO), para mejorar la precisión y robustez de los modelos en la detección de deepfakes de audio. Estas arquitecturas híbridas han mostrado una alta efectividad al permitir que los modelos capturen tanto las características espaciales como temporales del audio, lo cual es fundamental para detectar manipulación en tiempo real y con una precisión elevada (Al-Adwan et al., 2024).

\subsection{Estudios Previos en Detección de Deepfakes}
Varios estudios han analizado el desempeño de diferentes técnicas para detectar deepfakes. Por ejemplo, Yi et al. (2023) presentan una revisión de métodos de detección de deepfakes de audio, destacando cómo las características espectrales del habla (como el análisis de Mel-spectrogramas) pueden ser útiles para identificar anomalías en el audio. Además, los métodos basados en prosodia han demostrado ser efectivos para identificar patrones irregulares en deepfakes de audio, ya que los modelos generativos tienden a producir una prosodia "plana" o con variaciones artificiales.

Otro enfoque reciente es el uso de aprendizaje auto-supervisado, en el cual el modelo aprende a identificar patrones de autenticidad sin necesidad de una gran cantidad de datos etiquetados. Lanzino et al. (2024) proponen un método de auto-supervisión que permite a los modelos capturar patrones de voz natural y detectar deepfakes con mayor precisión. Este método es particularmente útil en escenarios donde los datos etiquetados son limitados o difíciles de obtener, y ha demostrado ser prometedor para mejorar la generalización de los modelos en la detección de deepfakes.

\subsection{Desafíos y Limitaciones en la Detección de Deepfakes de Audio}
A pesar de los avances, los sistemas de detección de deepfakes enfrentan varios desafíos. Uno de los principales problemas es la capacidad de generalización de los modelos, es decir, su habilidad para detectar deepfakes creados con nuevas técnicas que no han sido vistas durante el entrenamiento. Los modelos de detección tienden a especializarse en ciertos tipos de manipulación, lo que los hace vulnerables a ataques desconocidos. Esto plantea la necesidad de desarrollar modelos más robustos que puedan adaptarse a nuevas técnicas de generación de deepfakes (Heidari et al., 2023; Oorloff et al., 2024).

Otro desafío importante es la interpretabilidad de los modelos. Dado que muchos modelos de detección de deepfakes son complejos y funcionan como "cajas negras", es difícil para los investigadores y los usuarios entender cómo el modelo toma decisiones. La interpretabilidad es crucial en aplicaciones de seguridad y en contextos legales, donde es necesario explicar por qué un audio se clasifica como falso o auténtico (Yi et al., 2023).
