\subsection{Inteligencia Artificial (IA) y Aprendizaje Profundo}
La inteligencia artificial (IA) ha transformado la forma en que interactuamos con la tecnología, permitiendo que las máquinas realicen tareas que normalmente requieren habilidades humanas, como la interpretación del lenguaje, la visión y el reconocimiento de patrones. El aprendizaje profundo es una subdisciplina clave dentro de la IA que utiliza redes neuronales profundas (DNN) para analizar grandes volúmenes de datos y extraer patrones complejos (Goodfellow, Bengio, \& Courville, 2017). En lugar de depender de reglas explícitas programadas, las DNN aprenden directamente de los datos, permitiéndoles realizar predicciones precisas al procesar información no estructurada, como el audio. Este tipo de aprendizaje es especialmente útil en la detección de deepfakes, ya que permite que los modelos distingan entre voces reales y generadas artificialmente al identificar discrepancias en el tono, timbre y otros rasgos característicos de la voz humana (Mitchell, 2019).

Las redes neuronales profundas constan de múltiples capas, cada una de las cuales realiza una serie de cálculos para extraer características de mayor nivel de abstracción a partir de los datos de entrada. La representación jerárquica que logran estas redes permite identificar patrones complejos en señales de voz, tales como las fluctuaciones en el tono o las transiciones suaves entre fonemas, que son esenciales para reconocer la autenticidad de una grabación de voz. En el caso de los deepfakes de audio, el aprendizaje profundo facilita la detección de irregularidades en los patrones de voz generados artificialmente, lo cual es crucial para prevenir fraudes y proteger la identidad en el entorno digital (Mitchell, 2019).

\subsection{Procesamiento de Lenguaje Natural (PLN) y Procesamiento de Señales de Voz}
El procesamiento de lenguaje natural (PLN) se centra en la interacción entre computadoras y el lenguaje humano, permitiendo que las máquinas interpreten, generen y respondan al lenguaje de manera coherente. Cuando se trata de detección de deepfakes en audio, el PLN se extiende al análisis del habla, combinándose con el procesamiento de señales de voz, que transforma las ondas de sonido en datos digitales para su posterior análisis (Rao \& McMahan, 2019). A través de técnicas de PLN, es posible descomponer la señal de audio en sus elementos constitutivos (frecuencias, duraciones, intensidades) y analizar características como la entonación y el ritmo, que son fundamentales para identificar las particularidades de una voz auténtica (Rabiner \& Schafer, 2007).

La señal de voz se caracteriza por elementos acústicos que incluyen el tono, el timbre y la prosodia, cada uno de los cuales aporta información sobre el hablante y la autenticidad de su discurso. En el contexto de la detección de deepfakes, estos elementos permiten establecer patrones que las voces sintéticas encuentran difícil replicar con precisión. Por ejemplo, el tono y la prosodia pueden variar según el contexto y las emociones del hablante, una característica que suele perderse en las voces generadas artificialmente (Rabiner \& Schafer, 2007).

\subsection{Deepfakes de Audio}

Los deepfakes de audio representan una de las aplicaciones más avanzadas y potencialmente peligrosas del aprendizaje profundo. Estas falsificaciones de voz se logran mediante modelos generativos, como los modelos de redes neuronales recurrentes (RNN) o las redes de confrontación generativa (GAN), que pueden sintetizar voces convincentes a partir de muestras de audio de una persona específica. El resultado es un audio que imita la voz y el estilo de habla de una persona real, lo que puede ser usado en actividades fraudulentas, como la suplantación de identidad en llamadas telefónicas o la creación de grabaciones de voz falsas (Gomes-Gonçalves, 2022).

La creación de deepfakes de audio plantea desafíos éticos y de seguridad, ya que permite la manipulación de información de manera convincente, desafiando la autenticidad de las comunicaciones y poniendo en riesgo la seguridad de personas y organizaciones. Debido a que las voces humanas tienen características únicas y sutiles, los deepfakes en audio aún presentan fallos al intentar imitar todos los detalles acústicos, como los patrones de ritmo y la entonación, lo cual se convierte en una ventaja para los sistemas de detección basados en IA (Mitchell, 2019).

\subsection{Variables de Estudio para la Detección de Deepfakes en Audio}
En la detección de deepfakes de audio, se emplean variables acústicas clave que permiten analizar las diferencias entre una grabación genuina y una generada artificialmente. Cada una de estas variables proporciona información única sobre las características de la voz y son esenciales para identificar posibles manipulaciones.

\subsubsection{Tono de Voz (Pitch)}
El tono es la frecuencia fundamental de la voz y se relaciona con la percepción de si un sonido es grave o agudo. En la voz humana, el tono varía naturalmente según factores emocionales y fisiológicos, creando un patrón que es difícil de replicar exactamente en deepfakes. Los sistemas de detección utilizan el análisis de tono para observar irregularidades en estas variaciones, que pueden indicar una manipulación de la voz (Rabiner \& Schafer, 2007).

\subsubsection{Timbre de Voz}
El timbre se refiere a la "calidad" o "color" de la voz, influenciado por las características físicas del tracto vocal del hablante. Cada persona tiene un timbre único debido a diferencias en la estructura de sus cuerdas vocales y en la resonancia del tracto vocal. El timbre es especialmente importante en la detección de deepfakes, ya que los modelos de generación de voz suelen tener dificultades para imitar esta característica distintiva de forma precisa, lo que permite detectar falsificaciones a partir de anomalías en el timbre (Jurafsky \& Martin, 2024).

\subsubsection{Duración y Ritmo del Habla}
La duración de los sonidos y el ritmo general del habla son patrones consistentes en cada persona, lo que significa que un cambio en el ritmo puede indicar una manipulación. Las voces generadas tienden a presentar una cadencia que puede sonar artificial o "robotizada", especialmente cuando no logran replicar pausas o variaciones naturales en la duración de las sílabas y palabras (Rao \& McMahan, 2019).

\subsubsection{Mel-Spectrograma}
El mel-espectrograma es una herramienta visual que representa la energía de una señal de audio en diferentes frecuencias y permite identificar patrones espectrales característicos. Las voces reales suelen tener un espectrograma mel con transiciones suaves entre frecuencias, mientras que los deepfakes pueden mostrar patrones espectrales diferentes o artefactos debido a la síntesis. Analizar el mel-espectrograma permite a los modelos de IA detectar las irregularidades propias de las voces generadas (Rabiner \& Schafer, 2007).

\subsubsection{Prosodia (Entonación y Énfasis)}
La prosodia, que incluye variaciones en la entonación y el énfasis, proporciona un ritmo natural y una fluidez al habla humana. Estas variaciones son difíciles de imitar en las voces sintéticas, que tienden a tener una prosodia "plana" o inconsistente. Los sistemas de detección de deepfakes analizan la prosodia para identificar patrones irregulares que puedan indicar una generación artificial (Goodfellow et al., 2017).

\subsubsection{Articulación y Transiciones entre Fonemas}
La articulación de los sonidos del habla y las transiciones entre ellos son suaves en una voz humana. En los deepfakes, las transiciones entre fonemas pueden ser bruscas o presentar errores de articulación, debido a las limitaciones de los modelos de generación de voz. Este análisis permite detectar cuando una voz carece de la fluidez natural de la pronunciación humana, ayudando a identificar una grabación manipulada (Jurafsky \& Martin, 2024).
